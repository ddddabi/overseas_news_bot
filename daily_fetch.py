import feedparser
import pandas as pd
from datetime import datetime, timedelta
import argparse
from utils import (
    load_config,
    init_sheet,
    hash_entry,
    get_worksheet_by_title,
    send_webhook,
    append_rows_in_batches,
    to_kst
)
from googletrans import Translator
import requests
import logging
import os
from dateutil import parser as dtparser

# ë¡œê¹… ì„¤ì •
os.makedirs("logs", exist_ok=True)
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler("logs/daily_fetch.log", encoding='utf-8'),
        logging.StreamHandler()
    ]
)

def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--date", type=str, help="ë‹¨ì¼ ê¸°ì‚¬ ë‚ ì§œ (ì˜ˆ: 05-12)")
    parser.add_argument("--from_date", type=str, help="ì‹œì‘ ë‚ ì§œ (ì˜ˆ: 05-10)")
    parser.add_argument("--to_date", type=str, help="ì¢…ë£Œ ë‚ ì§œ (ì˜ˆ: 05-13)")
    return parser.parse_args()

def get_filter_dates(args):
    if args.date:
        return {args.date}
    elif args.from_date and args.to_date:
        from_dt = datetime.strptime(args.from_date, "%m-%d")
        to_dt = datetime.strptime(args.to_date, "%m-%d")
        return {
            (from_dt + timedelta(days=i)).strftime("%m-%d")
            for i in range((to_dt - from_dt).days + 1)
        }
    else:
        return {(to_kst(datetime.utcnow()) - timedelta(days=1)).strftime("%m-%d")}

def safe_parse(url, timeout=10):
    try:
        logging.info(f"â³ ìš”ì²­ ì¤‘: {url}")
        resp = requests.get(url, timeout=timeout)
        resp.raise_for_status()
        return feedparser.parse(resp.text)
    except Exception as e:
        logging.warning(f"âŒ ì‹¤íŒ¨: {url}\n   ì´ìœ : {e}")
        return None

def main():
    args = parse_args()
    cfg = load_config()
    send_webhook("âœ… [Daily Fetch] ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘", cfg)

    feeds = cfg["feeds"]["urls"]
    keywords = [kw.lower() for kw in cfg["keywords"]]
    translator = Translator()
    spreadsheet = init_sheet(cfg)

    sheet_all = get_worksheet_by_title(spreadsheet, "All News")
    sheet_filtered = get_worksheet_by_title(spreadsheet, "Filtered News")

    existing_filtered = sheet_filtered.get_all_records()
    existing_hashes = set(row.get("í•´ì‹œ") for row in existing_filtered if row.get("í•´ì‹œ"))

    search_time = to_kst(datetime.utcnow()).strftime('%m-%d %H:%M')
    filter_dates = get_filter_dates(args)

    all_rows = []
    filtered_rows = []

    for url in feeds:
        feed = safe_parse(url)
        if not feed:
            continue

        for entry in feed.entries:
            title = entry.title
            link = entry.link
            entry_hash = hash_entry(title, link)

            published = entry.get("published", "") or entry.get("updated", "")
            try:
                dt = dtparser.parse(published)
                kst_time = to_kst(dt)
                article_date = kst_time.strftime("%m-%d")
            except Exception as e:
                article_date = ""
                logging.warning(f"âš ï¸ ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: {published} â†’ {e}")

            try:
                direct = translator.translate(title, src='en', dest='ko').text
                natural = direct  # ìì—°ìŠ¤ëŸ¬ìš´ ì˜ì—­ ëŒ€ì‹  ì§ì—­ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            except Exception as e:
                logging.warning(f"âš ï¸ ë²ˆì—­ ì‹¤íŒ¨: {title}, ì—ëŸ¬: {e}")
                continue

            row = [search_time, article_date, title, direct, natural, link, entry_hash]
            all_rows.append(row)

            if article_date in filter_dates:
                if any(kw in title.lower() for kw in keywords):
                    if entry_hash not in existing_hashes:
                        filtered_rows.append(row)

    columns = ["ê²€ìƒ‰ì¼ì‹œ", "ê¸°ì‚¬ì¼ì", "ì œëª©(ì›ë¬¸)", "ì œëª©(ì§ì—­)", "ì œëª©(ì˜ì—­)", "ë§í¬", "í•´ì‹œ"]

    if all_rows:
        df_all = pd.DataFrame(all_rows, columns=columns)
        append_rows_in_batches(sheet_all, df_all.values.tolist())
        logging.info(f"ğŸ“š ì „ì²´ ë‰´ìŠ¤ {len(df_all)}ê±´ ì €ì¥ (All News)")
    else:
        logging.warning("â„¹ï¸ ì „ì²´ ë‰´ìŠ¤ ì—†ìŒ")

    num_filtered = len(filtered_rows)
    if num_filtered > 0:
        df_filtered = pd.DataFrame(filtered_rows, columns=columns)
        append_rows_in_batches(sheet_filtered, df_filtered.values.tolist())
        logging.info(f"âœ… í•„í„°ë§ëœ ë‰´ìŠ¤ {num_filtered}ê±´ ì €ì¥ (Filtered News)")
    else:
        logging.warning("âš ï¸ í•„í„°ë§ëœ ë‰´ìŠ¤ ì—†ìŒ")

    send_webhook(f"âœ… [Daily Fetch] ì „ì²´ {len(all_rows)}ê±´ ì¤‘ í•„í„°ë§ëœ {num_filtered}ê±´ ì €ì¥ ì™„ë£Œ", cfg)

if __name__ == "__main__":
    main()
